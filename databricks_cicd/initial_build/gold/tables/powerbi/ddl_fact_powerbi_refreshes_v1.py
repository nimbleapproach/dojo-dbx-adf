# Databricks notebook source
# Importing Libraries
import os
spark = spark  # noqa

# COMMAND ----------

ENVIRONMENT = os.environ["__ENVIRONMENT__"]
ENVIRONMENT

# COMMAND ----------


spark.catalog.setCurrentCatalog(f"gold_{ENVIRONMENT}")


# COMMAND ----------

catalog = spark.catalog.currentCatalog()
schema = 'powerbi'

# COMMAND ----------

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {catalog}.{schema}.fact_refreshes (
  RequestId STRING COMMENT 'The system generated Id for the activity',
  Id LONG COMMENT 'The id generated by the api call to collect the data',
  RefreshType STRING COMMENT 'The type of refresh that was attempted',
  StartTime TIMESTAMP COMMENT 'The datetime the refresh started',
  EndTime TIMESTAMP COMMENT 'The datetime the refresh started',
  Status STRING COMMENT 'The current status of the refresh',
  DatasetId STRING COMMENT 'The id of the dataset that was being refreshed',
  SysGoldInsertedDateTimeUTC TIMESTAMP COMMENT 'The datetime the activity was inserted into the gold layer',
  CONSTRAINT `RequestId` PRIMARY KEY (`RequestId`)
  )
USING delta
CLUSTER BY (DatasetId)
TBLPROPERTIES (
  'delta.checkpointPolicy' = 'v2',
  'delta.constraints.datewithinrange_start_datetime' = 'StartTime >= "1900-01-01"',
  'delta.enableDeletionVectors' = 'true',
  'delta.enableRowTracking' = 'true',
  'delta.feature.allowColumnDefaults' = 'supported',
  'delta.feature.checkConstraints' = 'supported',
  'delta.feature.columnMapping' = 'supported',
  'delta.feature.deletionVectors' = 'supported',
  'delta.feature.identityColumns' = 'supported',
  'delta.feature.invariants' = 'supported',
  'delta.feature.rowTracking' = 'supported',
  'delta.feature.v2Checkpoint' = 'supported')
""")
