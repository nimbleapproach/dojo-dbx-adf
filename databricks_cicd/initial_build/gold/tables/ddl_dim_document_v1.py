# Databricks notebook source
# Importing Libraries
import os
spark = spark  # noqa

# COMMAND ----------

ENVIRONMENT = os.environ["__ENVIRONMENT__"]
ENVIRONMENT

# COMMAND ----------


spark.catalog.setCurrentCatalog(f"gold_{ENVIRONMENT}")


# COMMAND ----------

catalog = spark.catalog.currentCatalog()
schema = 'orion'

# COMMAND ----------

# REMOVE ONCE SOLUTION IS LIVE
if ENVIRONMENT == 'dev':
    spark.sql(f"""
              DROP TABLE IF EXISTS {catalog}.{schema}.dim_document
              """)

# COMMAND ----------

spark.sql(f"""
CREATE TABLE IF NOT EXISTS {catalog}.{schema}.dim_document (
  document_pk BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
  local_document_id STRING NOT NULL COMMENT 'The unique id of the document within country',
  associated_document_id STRING COMMENT 'The unique id of any associated document, such as sales invoice doc for MSP, credit memo for MSP',
  document_date DATE COMMENT 'The date of the document',
  document_source STRING COMMENT 'The document source - credit memo, msp, quote, sales order, etc..',
  document_type STRING COMMENT 'The document type',
  document_status STRING COMMENT 'The document status',
  country_code STRING COMMENT 'The country code of the document',
  source_system_fk BIGINT COMMENT 'The ID from the Source System Dimension',
  bill_to_customer_no string COMMENT '',
  biztalkguid string COMMENT '',
  credit_memo  string COMMENT '',
  currency_code  string COMMENT '',
  currency_factor  decimal(10,4) COMMENT '',
  cust_gen_bus_postinggroup   string COMMENT '',
  cust_vatbus_postinggroup  string COMMENT '',
  customer_no string COMMENT '',
  dimension_set_id  string COMMENT '',
  due_date   timestamp COMMENT '',
  invoicebiztalkguid  string COMMENT '',
  invoice_title  string COMMENT '',
  mspusagesheaderbiztalkguid  string COMMENT '',
  order_date timestamp COMMENT '',
  order_no  string COMMENT '',
  posting_date timestamp COMMENT '',
  process_code   string COMMENT '',
  sales_credit_memo_no  string COMMENT '',
  sales_invoice_no string COMMENT '',
  sell_to_customer_no  string COMMENT '',
  shortcut_dimension1_code  string COMMENT '',
  sid bigint COMMENT '',
  vendor_dimension_value  string COMMENT '',
  vendor_account_id string COMMENT '',
  vendor_reference string COMMENT '',
  version_no  int COMMENT '',
  start_datetime TIMESTAMP NOT NULL COMMENT 'The dimensional start date of the record',
  end_datetime TIMESTAMP COMMENT 'The dimensional end date of the record, those records with a NULL value are current',
  is_current INT COMMENT 'Flag to indicate if this is the active dimension record per code',
  Sys_Gold_InsertedDateTime_UTC TIMESTAMP COMMENT 'The timestamp when this record was inserted into gold',
  Sys_Gold_ModifiedDateTime_UTC TIMESTAMP COMMENT 'The timestamp when this record was last updated in gold',
  CONSTRAINT `dim_document_primary_key` PRIMARY KEY (`document_pk`))
USING delta
CLUSTER BY (source_system_fk,document_source)
TBLPROPERTIES (
  'delta.checkpointPolicy' = 'v2',
  'delta.constraints.datewithinrange_start_datetime' = 'start_datetime >= "1900-01-01"',
  'delta.constraints.valid_is_current_value' = 'is_current IN ( 1 , 0 )',
  'delta.enableDeletionVectors' = 'true',
  'delta.enableRowTracking' = 'true',
  'delta.feature.allowColumnDefaults' = 'supported',
  'delta.feature.checkConstraints' = 'supported',
  'delta.feature.columnMapping' = 'supported',
  'delta.feature.deletionVectors' = 'supported',
  'delta.feature.identityColumns' = 'supported',
  'delta.feature.invariants' = 'supported',
  'delta.feature.rowTracking' = 'supported',
  'delta.feature.v2Checkpoint' = 'supported')
""")

# COMMAND ----------

# Add in the UNKNOWN Member
sqldf= spark.sql(f"""
SELECT CAST(-1 AS BIGINT) AS document_pk,
       CAST('N/A' AS STRING) AS local_document_id,
       CAST('N/A' AS STRING) AS associated_document_id,
       CAST('1900-01-01' AS DATE) AS document_date,
       CAST('N/A' AS STRING) AS document_type,
       CAST('N/A' AS STRING) AS document_status,
       CAST('N/A' AS STRING) AS vendor_account_id,
       CAST(NULL AS STRING) AS country_code,
       CAST(-1 AS BIGINT) AS source_system_fk,
       CAST('1900-01-01' AS TIMESTAMP) AS start_datetime,
       CAST('9999-12-31' AS TIMESTAMP) AS end_datetime,
       CAST(1 AS INTEGER) AS is_current,
       CAST(NULL AS TIMESTAMP) AS Sys_Gold_InsertedDateTime_UTC,
       CAST(NULL AS TIMESTAMP) AS Sys_Gold_ModifiedDateTime_UTC
WHERE NOT EXISTS ( SELECT 1 FROM {catalog}.{schema}.dim_document WHERE document_pk = -1 AND source_system_fk = -1)
""").write.mode("overwrite").option("mergeSchema", "true").saveAsTable(f"{catalog}.{schema}.dim_document")
