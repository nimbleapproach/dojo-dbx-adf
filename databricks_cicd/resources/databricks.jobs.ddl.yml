# Databricks Jobs Configuration.
# This file defines the jobs and their configurations for deployment to Databricks.
# Each job can represent a notebook, library, or other executable entity within Databricks.
## for additional options please see schema.json in that repository (can be generated by "databricks bundle schema" command)
resources:
  jobs:
    inf_edw_model_job_deploy_ddl: # Provide a unique name for this bundle. This name should differentiate it from other projects.
      name: inf_edw_model_job_deploy_ddl # Provide a unique name for this job. This name should differentiate it from other projects.
      
      email_notifications:
        on_failure:
          - ${var.email_team_notification}
          - ${var.email_mailbox_notification}
        no_alert_for_skipped_runs: false
      permissions:
        - group_name: users
          level: CAN_MANAGE_RUN
      
      tasks:
        - task_key: deploy_ddl
          description: task to deploy DDLs 
          notebook_task:
            notebook_path: ../notebooks/deploy_ddl.py
            base_parameters:
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
          existing_cluster_id: ${var.existing_cluster_id}
          max_retries: 1
          min_retry_interval_millis: 100000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: fsspec
          
        - task_key: run_tests
          description: run all unit tests
          depends_on: 
            - task_key: deploy_ddl
          notebook_task:
            notebook_path: ../notebooks/run_tests.py
          existing_cluster_id: ${var.existing_cluster_id}
          max_retries: 1
          min_retry_interval_millis: 100000
          retry_on_timeout: False
          libraries:
            - pypi:
                package: databricks-sdk
            - pypi:
                package: pytest