{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime as dt\n",
    "import dlt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "catalog = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"task_start\", key=\"catalog\", debugValue=\"dev\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.view\n",
    "def taxi_raw():\n",
    "  return spark.read.format(\"json\").load(\"/databricks-datasets/nyctaxi/sample/json/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destination table to which we append flows\n",
    "dlt.create_streaming_table(\n",
    "    name=\"filtered_taxis\",\n",
    "    comment=\"taxi data\",\n",
    "    table_properties={\"pipelines.autoOptimize.zOrderCols\": \"snapshot_dt\"},\n",
    "    partition_cols=[\"VendorID\"],\n",
    "    schema=\"DOLocationID BIGINT,  PULocationID BIGINT,  RatecodeID BIGINT,  VendorID BIGINT,  congestion_surcharge DOUBLE,  extra DOUBLE,  fare_amount DOUBLE,  improvement_surcharge DOUBLE,  mta_tax DOUBLE,  passenger_count BIGINT,  payment_type BIGINT,  store_and_fwd_flag STRING,  tip_amount DOUBLE,  tolls_amount DOUBLE,  total_amount DOUBLE,  tpep_dropoff_datetime STRING,  tpep_pickup_datetime STRING,  trip_distance DOUBLE,  pep_pickup_date_txt DATE\",\n",
    "    expect_all_or_drop={\n",
    "        \"PULocationID present\": \"PULocationID > 0\",\n",
    "        \"fare_amount greater than 0\": \"fare_amount > 0\",\n",
    "        \"pep_pickup_date_txt from 2020 onwards\": \"pep_pickup_date_txt > date'2020-01-01'\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Create and append the flow for each table and segment ID, as append_flow is decorator I had to use evaluate as name of the function has to be diffrent every time.\n",
    "    # soon parameter name= will be available (it is already in docuemntation) so it will be possible to remove exec()\n",
    "@dlt.append_flow(target=\"filtered_taxis\")\n",
    "def flow_filtered_taxis():\n",
    "    return (\n",
    "    spark.readStream.format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .table(f\"dojo_gold.filtered_taxis\")\n",
    "    .filter(\"_change_type IN ('insert')\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name=\"filtered_taxis\")\n",
    "def filtered_taxis():\n",
    "  return dlt.read(\"taxi_raw\").filter(F.expr(\"fare_amount < 30\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
