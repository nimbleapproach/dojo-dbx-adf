# Databricks notebook source
import os

ENVIRONMENT = os.environ["__ENVIRONMENT__"]

# COMMAND ----------

spark.catalog.setCurrentCatalog(f"gold_{ENVIRONMENT}")

# COMMAND ----------

# MAGIC %sql
# MAGIC
# MAGIC -- Databricks notebook source
# MAGIC -- MAGIC %md
# MAGIC -- MAGIC Widgets are used to give Data Factory a way to hand over parameters. In that we we can control the environment.
# MAGIC -- MAGIC If there is no widget defined, Data Factory will automatically create them.
# MAGIC -- MAGIC For us while developing we can use the try and excep trick here.
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC -- MAGIC %python
# MAGIC -- MAGIC import os
# MAGIC -- MAGIC
# MAGIC -- MAGIC ENVIRONMENT = os.environ["__ENVIRONMENT__"]
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC -- MAGIC %md
# MAGIC -- MAGIC The target catalog depens on the enivronment. Since we are using Unity Catalog we need to use a unqiue name for the catalog. This is the reason why we name the dev silver catalog "silver_dev" for example.
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC -- MAGIC %python
# MAGIC -- MAGIC spark.catalog.setCurrentCatalog(f"silver_{ENVIRONMENT}")
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC USE SCHEMA tag02;
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC CREATE OR REPLACE TABLE dim_cost_centre
# MAGIC   ( dim_cost_centre_pk bigint
# MAGIC     GENERATED BY DEFAULT AS IDENTITY
# MAGIC     ,cost_centre_code STRING NOT NULL 
# MAGIC       COMMENT 'Cost Centre Code'
# MAGIC     ,cost_centre_name STRING
# MAGIC       COMMENT 'The descriptive name of the associated cost centre'
# MAGIC     ,cost_centre_hash_key STRING
# MAGIC       COMMENT 'Hash value of the dimensional attributes per cost centre code'
# MAGIC     ,start_datetime TIMESTAMP NOT NULL 
# MAGIC       COMMENT 'The dimensional start date of the record'
# MAGIC     ,end_datetime TIMESTAMP
# MAGIC       COMMENT 'The dimensional end date of the record, those with a NULL value is curent'                      
# MAGIC     ,is_current INTEGER
# MAGIC       COMMENT 'Flag to indicate if this is the active dimension record per code'
# MAGIC     ,Sys_Gold_InsertedDateTime_UTC TIMESTAMP
# MAGIC       COMMENT 'The timestamp when this record was inserted into gold'
# MAGIC     ,Sys_Gold_ModifiedDateTime_UTC TIMESTAMP
# MAGIC       COMMENT 'The timestamp when this record was last updated in gold'      
# MAGIC ,CONSTRAINT dim_cost_centre_primary_key PRIMARY KEY(dim_cost_centre_pk)
# MAGIC   )
# MAGIC
# MAGIC TBLPROPERTIES ('delta.feature.allowColumnDefaults' = 'supported')
# MAGIC CLUSTER BY (cost_centre_code);
# MAGIC
# MAGIC -- COMMAND ----------
# MAGIC
# MAGIC ALTER TABLE dim_cost_centre ADD CONSTRAINT dateWithinRange_start_datetime CHECK (start_datetime >= '1900-01-01');
# MAGIC ALTER TABLE dim_cost_centre ADD CONSTRAINT valid_is_current_value CHECK (is_current IN ('1','0'));
# MAGIC

# COMMAND ----------

# MAGIC %md
# MAGIC **Now the table is created we want to insert the unknown member...**

# COMMAND ----------

# MAGIC %py
# MAGIC
# MAGIC sqldf= spark.sql("""
# MAGIC SELECT CAST(-1 AS BIGINT) AS dim_cost_centre_pk,
# MAGIC        CAST('N/A' AS STRING) AS cost_centre_code,
# MAGIC        CAST(NULL AS STRING) AS cost_centre_name,
# MAGIC        CAST(NULL AS STRING) AS cost_centre_hash_key,
# MAGIC        CAST('1900-01-01' AS TIMESTAMP) AS start_datetime,
# MAGIC        CAST(NULL AS TIMESTAMP) AS end_datetime,
# MAGIC        CAST(1 AS INTEGER) AS is_current,
# MAGIC        CAST(NULL AS TIMESTAMP) AS Sys_Gold_InsertedDateTime_UTC,
# MAGIC        CAST(NULL AS TIMESTAMP) AS Sys_Gold_ModifiedDateTime_UTC       
# MAGIC """)
# MAGIC
# MAGIC #display(sqldf)
# MAGIC sqldf.write.mode("append").option("mergeSchema", "true").saveAsTable(f"gold_{ENVIRONMENT}.tag02.dim_cost_centre")