-- Databricks notebook source
-- MAGIC %python
-- MAGIC import os
-- MAGIC
-- MAGIC ENVIRONMENT = os.environ["__ENVIRONMENT__"]

-- COMMAND ----------

-- MAGIC %md
-- MAGIC The target catalog depens on the enivronment. Since we are using Unity Catalog we need to use a unqiue name for the catalog. This is the reason why we name the dev silver catalog "silver_dev" for example.

-- COMMAND ----------

-- MAGIC %python
-- MAGIC spark.catalog.setCurrentCatalog(f"gold_{ENVIRONMENT}")

-- COMMAND ----------

CREATE SCHEMA if not EXISTS orion

-- COMMAND ----------

USE SCHEMA orion

-- COMMAND ----------
CREATE OR REPLACE TABLE dim_vendor 
 (
  vendor_pk BIGINT NOT NULL GENERATED BY DEFAULT AS IDENTITY (START WITH 1 INCREMENT BY 1),
  vendor_code STRING NOT NULL COMMENT 'Vendor Code',
  vendor_name_internal STRING COMMENT 'The internal name text of the vendor',
  country_code  STRING COMMENT 'The country code of the vendor',
  local_vendor_id STRING COMMENT 'The local ID from the source system if exists',
  source_system_id BIGINT COMMENT 'The ID from the Source System Dimension',
  --vendor_hash_key STRING COMMENT 'Hash value of the dimensional attributes per vendor code',
  start_datetime TIMESTAMP NOT NULL COMMENT 'The dimensional start date of the record',
  end_datetime TIMESTAMP COMMENT 'The dimensional end date of the record, those records with a NULL value are current',
  is_current INT COMMENT 'Flag to indicate if this is the active dimension record per code',
  Sys_Gold_InsertedDateTime_UTC TIMESTAMP COMMENT 'The timestamp when this record was inserted into gold',
  Sys_Gold_ModifiedDateTime_UTC TIMESTAMP COMMENT 'The timestamp when this record was last updated in gold',
  CONSTRAINT `dim_vendor_primary_key` PRIMARY KEY (`vendor_pk`))
USING delta
CLUSTER BY (source_system_id,vendor_code)
TBLPROPERTIES (
  'delta.checkpointPolicy' = 'v2',
  'delta.constraints.datewithinrange_start_datetime' = 'start_datetime >= \'1900-01-01\'',
  'delta.constraints.valid_is_current_value' = 'is_current IN ( 1 , 0 )',
  'delta.enableDeletionVectors' = 'true',
  'delta.enableRowTracking' = 'true',
  'delta.feature.allowColumnDefaults' = 'supported',
  'delta.feature.checkConstraints' = 'supported',
  'delta.feature.columnMapping' = 'supported',
  'delta.feature.deletionVectors' = 'supported',
  'delta.feature.identityColumns' = 'supported',
  'delta.feature.invariants' = 'supported',
  'delta.feature.rowTracking' = 'supported',
  'delta.feature.v2Checkpoint' = 'supported')
;
-- COMMAND ----------

ALTER TABLE dim_vendor ADD CONSTRAINT dateWithinRange_Bronze_InsertDateTime CHECK (Sys_Gold_InsertDateTime_UTC >= '1900-01-01');
ALTER TABLE dim_vendor ADD CONSTRAINT dateWithinRange_Silver_InsertDateTime CHECK (Sys_Gold_ModifedDateTime_UTC >= '1900-01-01');

-- MAGIC # dim_vendor default member
-- MAGIC sqldf= spark.sql("""
-- MAGIC SELECT CAST(-1 AS BIGINT) AS vendor_pk,
-- MAGIC        CAST('N/A' AS STRING) AS vendor_code,
-- MAGIC        CAST(NULL AS STRING) AS vendor_name_internal,
-- MAGIC        CAST(NULL AS STRING) AS country_code,
-- MAGIC        CAST(NULL AS STRING) AS local_vendor_id,
-- MAGIC        CAST(-1 AS BIGINT) AS source_system_id,
-- MAGIC        CAST('1900-01-01' AS TIMESTAMP) AS start_datetime,
-- MAGIC        CAST(NULL AS TIMESTAMP) AS end_datetime,
-- MAGIC        CAST(1 AS INTEGER) AS is_current,
-- MAGIC        CAST(NULL AS TIMESTAMP) AS Sys_Gold_InsertedDateTime_UTC,
-- MAGIC        CAST(NULL AS TIMESTAMP) AS Sys_Gold_ModifiedDateTime_UTC
-- MAGIC """).write.mode("append").option("mergeSchema", "true").saveAsTable("dim_vendor")
